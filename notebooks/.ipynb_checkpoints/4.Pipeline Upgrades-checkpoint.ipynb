{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40a74a2-ba20-4183-b1e1-7983eaedef7c",
   "metadata": {},
   "source": [
    "# Pipeline Upgrades\n",
    "\n",
    "In this section, we look at how we can use the previous building blocks that we put together to test different pipeline combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25793b-475d-43f0-b449-1544df9e1ead",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "In our previous example, we saw how we could evaluate a RAG system using some simple metrics such as recall. Let's see how we can use this to quickly experiment with different pipelines and get a quantiative idea of how good or bad different metrics stack up against each other\n",
    "\n",
    "> Make sure to have set the environment variable `COHERE_API_KEY` in your shell for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa1dd8a-2ac9-4e85-9574-3d718c6fbee6",
   "metadata": {},
   "source": [
    "### Re-Rankers\n",
    "\n",
    "How do re-rankers affect the result of the final returned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce9db59-bdcb-4647-8699-4f21bef5c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data import get_labels\n",
    "from lib.db import get_table\n",
    "from lib.models import EmbeddedPassage\n",
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"../lance\")\n",
    "test_data = get_labels(\"../queries_single_label.json\")\n",
    "table = get_table(db,\"ms_marco\",EmbeddedPassage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ed58f7-05a0-4e5d-801f-be76bbc45f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@3': 1.0,\n",
       " 'mrr@5': 1.0,\n",
       " 'mrr@10': 1.0,\n",
       " 'mrr@15': 1.0,\n",
       " 'mrr@25': 1.0,\n",
       " 'recall@3': 1.0,\n",
       " 'recall@5': 1.0,\n",
       " 'recall@10': 1.0,\n",
       " 'recall@15': 1.0,\n",
       " 'recall@25': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lancedb.rerankers import CohereReranker\n",
    "from lib.eval import score\n",
    "\n",
    "def rerank_search(query,table,limit,query_type):\n",
    "    reranker = CohereReranker()\n",
    "    return [\n",
    "        item[\"chunk_id\"]\n",
    "        for item in table.search(query, query_type=query_type).limit(limit).rerank(reranker=reranker).to_list()\n",
    "    ]\n",
    "\n",
    "query = test_data[0]\n",
    "retrieved_chunk_ids = rerank_search(query['query'],table,25,\"fts\")\n",
    "evaluation_metrics = score(retrieved_chunk_ids,query['selected_chunk_id'])\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7e7990b-165f-4d98-adb2-9f1344a42ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 10/10 [00:00<00:00, 43.82it/s]\n",
      "100%|████████████████████████████████████████████| 10/10 [00:00<00:00, 43.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "|           |   Full Text Search |   Semantic Search |   Reranked Full Text Search |   Reranked Semantic |\n",
      "+===========+====================+===================+=============================+=====================+\n",
      "| mrr@3     |               0.25 |               0.5 |                        0.67 |                0.67 |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| mrr@5     |               0.34 |               0.5 |                        0.69 |                0.69 |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| mrr@10    |               0.37 |               0.5 |                        0.69 |                0.69 |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| mrr@15    |               0.37 |               0.5 |                        0.69 |                0.69 |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| mrr@25    |               0.37 |               0.5 |                        0.69 |                0.69 |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| recall@3  |               0.4  |               1   |                        0.9  |                0.9  |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| recall@5  |               0.8  |               1   |                        1    |                1    |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| recall@10 |               1    |               1   |                        1    |                1    |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| recall@15 |               1    |               1   |                        1    |                1    |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n",
      "| recall@25 |               1    |               1   |                        1    |                1    |\n",
      "+-----------+--------------------+-------------------+-----------------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from lib.query import full_text_search,semantic_search\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "test_data = get_labels(\"../queries_single_label.json\")[:10]\n",
    "\n",
    "def rerank_semantic(table,queries,top_k):\n",
    "    return [rerank_search(query['query'],table,top_k,\"vector\") for query in queries]\n",
    "\n",
    "def rerank_fts(table,queries,top_k):\n",
    "    return [rerank_search(query['query'],table,top_k,\"fts\") for query in queries]\n",
    "\n",
    "candidates = {\n",
    "    \"Full Text Search\": full_text_search,\n",
    "    \"Semantic Search\": semantic_search,\n",
    "    \"Reranked Full Text Search\" : rerank_fts,\n",
    "    \"Reranked Semantic\":rerank_semantic\n",
    "}\n",
    "results = {}\n",
    "\n",
    "for candidate,search_fn in candidates.items():\n",
    "    search_results = search_fn(table,test_data,25)\n",
    "    evaluation_metrics = [\n",
    "        score(retrieved_chunk_ids,query['selected_chunk_id']) \n",
    "        for retrieved_chunk_ids,query in zip(search_results,test_data)\n",
    "    ]\n",
    "    results[candidate] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17cf55-370b-4ba5-b9ac-a9ee42cd45a3",
   "metadata": {},
   "source": [
    "## Metadata Ingestion\n",
    "\n",
    "Now let's see how we can use some metadata to improve our retrieval performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15778e-417f-4d64-9ba3-d5a1c79bcf1c",
   "metadata": {},
   "source": [
    "## Embedding Questions\n",
    "\n",
    "We previously used Instructor to generate synthethic questions and answers. Let's see how we can expand on this to improve our retrieval pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75014b65-2992-4adb-a532-4b3d6addc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"../lance\")\n",
    "table = db.open_table(\"ms_marco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2d7c12-0c9a-4510-9e99-71c6803b3902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       " \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = table.to_pandas()['text'].tolist()\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9190b60f-aa26-4187-8cf9-b502f84804d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "class EmbeddedText(LanceModel):\n",
    "    vector: Vector(dim=func.ndims()) = func.VectorField()  # type: ignore\n",
    "    chunk_id: str\n",
    "    text: str = func.SourceField()\n",
    "    source_text: str\n",
    "\n",
    "\n",
    "table_withqa = db.create_table(\"ms_marco_v2\", schema=EmbeddedText,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a6d6e76-ac2a-422a-a122-aba91b316f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 814/814 [02:41<00:00,  5.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.synthethic import generate_question_batch\n",
    "\n",
    "questions = await generate_question_batch(chunks,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1af470cb-9d56-4b43-81e5-95abff9a01e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels saved to ../synth_questions.jsonl\n"
     ]
    }
   ],
   "source": [
    "from lib.data import save_labels\n",
    "def get_questions(questions):\n",
    "    for row in questions:\n",
    "        for question in row[\"response\"]:\n",
    "            yield {\n",
    "                \"question\":question.question,\n",
    "                \"source\":row[\"source\"]\n",
    "            }\n",
    "\n",
    "save_labels(get_questions(questions),\"../synth_questions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acd6699c-6c1a-456d-b1e5-705557bb1891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163it [04:04,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from lib.data import get_labels\n",
    "from itertools import batched\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "data = get_labels(\"../synth_questions.jsonl\")\n",
    "\n",
    "def question_generator(questions):\n",
    "    for row in data:\n",
    "        chunk_id = hashlib.md5(row['source'].encode()).hexdigest()\n",
    "        question_chunk_id = hashlib.md5(row['question'].encode()).hexdigest()\n",
    "        yield {\n",
    "            'chunk_id': chunk_id,\n",
    "            'source_text':row['source'],\n",
    "            'text': row['source']\n",
    "        }\n",
    "        yield {\n",
    "            'chunk_id': chunk_id,\n",
    "            'source_text':row['source'],\n",
    "            'text': row['question']\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "batched_data = batched(question_generator(data),30)\n",
    "\n",
    "for batch in tqdm(batched_data):\n",
    "    table_withqa.add(list(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7ec8ba7-06b6-474c-9cfa-826ee9174b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_withqa.create_fts_index(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4aa2d7ec-e014-4f7f-a449-6cef3c45ce3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 111/111 [00:06<00:00, 17.57it/s]\n",
      "100%|██████████████████████████████████████████| 111/111 [00:02<00:00, 47.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------------+--------------------------------+\n",
      "|           |   Semantic Search on Metadata |   Full Text Search on Metadata |\n",
      "+===========+===============================+================================+\n",
      "| mrr@3     |                          0.33 |                           0.3  |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| mrr@5     |                          0.36 |                           0.33 |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| mrr@10    |                          0.38 |                           0.36 |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| mrr@15    |                          0.39 |                           0.37 |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| mrr@25    |                          0.4  |                           0.38 |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| recall@3  |                          0.44 |                           0.35 |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| recall@5  |                          0.59 |                           0.49 |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| recall@10 |                          0.76 |                           0.67 |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| recall@15 |                          0.84 |                           0.81 |\n",
      "+-----------+-------------------------------+--------------------------------+\n",
      "| recall@25 |                          0.96 |                           0.9  |\n",
      "+-----------+-------------------------------+--------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.query import semantic_search,full_text_search\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from lib.eval import score\n",
    "\n",
    "test_data = get_labels(\"../queries_single_label.json\")\n",
    "\n",
    "candidates = {\n",
    "    \"Semantic Search on Metadata\": semantic_search,\n",
    "    \"Full Text Search on Metadata\": full_text_search\n",
    "}\n",
    "\n",
    "table_withqa = db.open_table(\"ms_marco_v2\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for candidate,search_fn in candidates.items():\n",
    "    search_results = search_fn(table_withqa,test_data,25)\n",
    "    evaluation_metrics = [\n",
    "        score(retrieved_chunk_ids,query['selected_chunk_id']) \n",
    "        for retrieved_chunk_ids,query in zip(search_results,test_data)\n",
    "    ]\n",
    "    results[candidate] = pd.DataFrame(evaluation_metrics).mean()\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(df.round(2), headers='keys', tablefmt='grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vibecheck",
   "language": "python",
   "name": "vibecheck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
