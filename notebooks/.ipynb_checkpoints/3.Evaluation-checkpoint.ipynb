{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189ad354-fd21-4109-9324-b5a3da46fb3e",
   "metadata": {},
   "source": [
    "# Evaluating Our Data\n",
    "\n",
    "Let's see an example of how we can evaluate our data using the ms-marco dataset with real user queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351613e1-63d6-423f-be48-29ac4dd2cd46",
   "metadata": {},
   "source": [
    "## Using a real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5a8945-cd6a-40a7-a682-4ea27b82bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "SIZES = [3, 5, 10, 15, 25]\n",
    "\n",
    "def calculate_recall(predictions, labels):\n",
    "    correct_predictions = sum(1 for label in labels if label in predictions)\n",
    "    if labels:\n",
    "        return correct_predictions / len(labels)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def calculate_reciprocal_rank(predictions, labels):\n",
    "    for index, prediction in enumerate(predictions):\n",
    "        if prediction in labels:\n",
    "            return 1 / (index + 1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "metrics = {\"mrr\": calculate_reciprocal_rank, \"recall\": calculate_recall}\n",
    "\n",
    "\n",
    "def score(preds, label):\n",
    "    return {\n",
    "        f\"{fn_name}@{size}\": round(metrics[fn_name](preds[:size], [label]), 3)\n",
    "        for fn_name, size in product(metrics.keys(), SIZES)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce3aeefb-5f92-40f8-80ac-78193b0c8cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 111/111 [00:02<00:00, 45.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mrr@3        0.345306\n",
       "mrr@5        0.391703\n",
       "mrr@10       0.416468\n",
       "mrr@15       0.419622\n",
       "mrr@25       0.420027\n",
       "recall@3     0.522523\n",
       "recall@5     0.729730\n",
       "recall@10    0.909910\n",
       "recall@15    0.945946\n",
       "recall@25    0.954955\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.data import get_labels\n",
    "from tqdm import tqdm\n",
    "from lib.query import full_text_search\n",
    "from lib.models import EmbeddedPassage\n",
    "import lancedb\n",
    "from lib.db import get_table\n",
    "import pandas as pd\n",
    "\n",
    "db = lancedb.connect(\"../lance\")\n",
    "data = get_labels(\"../queries_single_label.json\")\n",
    "table = get_table(db,\"ms_marco\",EmbeddedPassage)\n",
    "search_results = full_text_search(table,data,25)\n",
    "evaluation_metrics = [\n",
    "    score(retrieved_chunk_ids,query['selected_chunk_id']) \n",
    "    for retrieved_chunk_ids,query in zip(search_results,data)\n",
    "]\n",
    "pd.DataFrame(evaluation_metrics).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214575c9-342e-49c0-87d7-fc5aa14c7d56",
   "metadata": {},
   "source": [
    "## Cold Starting with Instructor\n",
    "\n",
    "What can we do if we have no user queries and we're just starting out? Well, the easiest way is to use synthethic queries to automatically generate the data to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d979daf0-a308-4d1a-a53c-aea7711a1b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input': 'What distinguishes conversion disorder from factitious disorders and malingering?',\n",
       "  'source': 'Conversion disorder is a type of somatoform disorder where physical symptoms or signs are present that cannot be explained by a medical condition. Very importantly, unlike factitious disorders and malingering, the symptoms of somatoform disorders are not intentional or under conscious control of the patient'},\n",
       " {'input': 'What are some characteristics that set conifers apart from other types of woody plants?',\n",
       "  'source': 'A conifer is a tree or shrub which produces distinctive cones as part of its sexual reproduction. These woody plants are classified among the gymnosperms, and they have a wide variety of uses, from trapping carbon in the environment to providing resins which can be used in the production of solvents. Several features beyond the cones set conifers apart from other types of woody plants. A conifer is typically evergreen, although some individuals are deciduous, and almost all conifers have needle or scale-like leaves'},\n",
       " {'input': 'What are the key characteristics and care tips for Dascyllus aruanus, also known as three-striped damselfish, as described in the text?',\n",
       "  'source': 'Known by multiple common names, such as humbug damselfish, three-striped damselfish and white-tailed damselfish, Dascyllus aruanus is a feisty little fish that adapts well to aquarium life. Three-striped damselfish can be pugnacious and are better introduced at the latter stages of setting up a marine fish community. Remove as many of the three-striped damselfish fry as you want to try and raise to a rearing aquarium, with an absence of adult fish and invertebrates that might look upon the young fish as tasty morsels for the taking. Dascyllus aruanus is a worthy first-time breeding project for up-and-coming marine aquarists'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import instructor\n",
    "import openai\n",
    "from pydantic import BaseModel,Field\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())\n",
    "\n",
    "class QuestionAnswerPair(BaseModel):\n",
    "    \"\"\"\n",
    "    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n",
    "    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n",
    "    was derived from the question.\n",
    "    \"\"\"\n",
    "\n",
    "    chain_of_thought: str = Field(\n",
    "        ..., description=\"The reasoning process leading to the answer.\"\n",
    "    )\n",
    "    question: str = Field(\n",
    "        ..., description=\"The generated question from the text chunk.\"\n",
    "    )\n",
    "    answer: str = Field(..., description=\"The answer to the generated question.\")\n",
    "\n",
    "async def generate_question_batch(text_chunk_batch):\n",
    "    async def generate_question(text: str):\n",
    "        question = await client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": f\"Here is the text chunk : {text}\"},\n",
    "            ],\n",
    "            response_model=QuestionAnswerPair,\n",
    "            max_retries=3,\n",
    "        )\n",
    "        return (question,text)\n",
    "\n",
    "    coros = [\n",
    "        generate_question(item) for item in text_chunk_batch\n",
    "    ]\n",
    "    res = await asyncio.gather(*coros)\n",
    "    return [{\"input\": item.question, \"source\": text} for item,text in res]\n",
    "\n",
    "chunks = [\n",
    "    \"Conversion disorder is a type of somatoform disorder where physical symptoms or signs are present that cannot be explained by a medical condition. Very importantly, unlike factitious disorders and malingering, the symptoms of somatoform disorders are not intentional or under conscious control of the patient\",\n",
    "    \"A conifer is a tree or shrub which produces distinctive cones as part of its sexual reproduction. These woody plants are classified among the gymnosperms, and they have a wide variety of uses, from trapping carbon in the environment to providing resins which can be used in the production of solvents. Several features beyond the cones set conifers apart from other types of woody plants. A conifer is typically evergreen, although some individuals are deciduous, and almost all conifers have needle or scale-like leaves\",\n",
    "    \"Known by multiple common names, such as humbug damselfish, three-striped damselfish and white-tailed damselfish, Dascyllus aruanus is a feisty little fish that adapts well to aquarium life. Three-striped damselfish can be pugnacious and are better introduced at the latter stages of setting up a marine fish community. Remove as many of the three-striped damselfish fry as you want to try and raise to a rearing aquarium, with an absence of adult fish and invertebrates that might look upon the young fish as tasty morsels for the taking. Dascyllus aruanus is a worthy first-time breeding project for up-and-coming marine aquarists\"\n",
    "]\n",
    "\n",
    "questions = await generate_question_batch(chunks)\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20284be-9d51-4638-bb3f-abeee207a559",
   "metadata": {},
   "source": [
    "### What is Instructor?\n",
    "\n",
    "Instructor is a library that provides structured output validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d190ae68-1066-437a-846d-d7916395e50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Define your desired output structure\n",
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "# Patch the OpenAI client\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "# Extract structured data from natural language\n",
    "user_info = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserInfo,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n",
    ")\n",
    "\n",
    "print(user_info.name)\n",
    "print(user_info.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45512a-2240-4a50-9bf1-49adfef7b566",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Now that we've seen what Instructor can do, let's work through a few different exercises to get a better understanding of the library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e38cc6-72a3-40b2-b8c0-b5f9f991cb2b",
   "metadata": {},
   "source": [
    "#### Adding some docstrings\n",
    "\n",
    "Let's try creating a Pydantic Model that has docstrings and descriptions using the `Field` object.\n",
    "\n",
    "Modify the original `UserInfo` object to include a docstring and a description of each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "461c3ba8-770b-4e36-a0c0-8faabd43af55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"This is a model which represents a single user's information\",\n",
       " 'properties': {'name': {'description': \"This is the user's name which we have extracted\",\n",
       "   'title': 'Name',\n",
       "   'type': 'string'},\n",
       "  'age': {'description': \"This is the user's age which we have extracted\",\n",
       "   'title': 'Age',\n",
       "   'type': 'integer'}},\n",
       " 'required': ['name', 'age'],\n",
       " 'title': 'UserInfo',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class UserInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    This is a model which represents a single user's information\n",
    "    \"\"\"\n",
    "    name: str = Field(...,description=\"This is the user's name which we have extracted\")\n",
    "    age: int = Field(...,description=\"This is the user's age which we have extracted\")\n",
    "\n",
    "UserInfo.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93cafc1-677d-4da5-83b5-2a84d094ecd7",
   "metadata": {},
   "source": [
    "#### Using simple validation\n",
    "\n",
    "Now that we've seen how to work with simple User Fields, let's start implementing validators. \n",
    "\n",
    "Validators are simple functions that run on the returned response from OpenAI. Using Validators, we can ensure that we have valid output. To show how a simple validator might work, let's try to implement a simple function which generates three categories given an article title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de0c48c9-2c4b-4ae1-ba5e-e5bfd05f9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_validator\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"\n",
    "    This is a model which represents a list of categories that we can classify the given article into\n",
    "    \"\"\"\n",
    "    categories: list[str] = Field(..., description=\"This is the list of categories that we can classify the given article into\")\n",
    "    keywords: list[str] = Field(...,description=\"These are some keywords that users might search for when looking for similar articles as the given article.\")\n",
    "\n",
    "    @field_validator('categories')\n",
    "    def check_categories_length(cls, v):\n",
    "        if not (3 <= len(v) <= 5):\n",
    "            raise ValueError('categories must have at least 3 elements and at most 5 elements')\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0869c726-448f-426d-990b-aac2f0765db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metadata(categories=['Technology', 'Healthcare', 'Artificial Intelligence'], keywords=['Future', 'Artificial Intelligence', 'Healthcare'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=Metadata,\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a World Class classification Algorithm. You are about to be given an article title to classify. Make sure to return your response in the model provided\"},\n",
    "             {\"role\": \"user\", \"content\": \"Give me a sample article title for classification: The Future of Artificial Intelligence in Healthcare\"}\n",
    "             ],\n",
    ")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7fecb-796e-455c-84ee-46d703897287",
   "metadata": {},
   "source": [
    "#### More Complex Types\n",
    "\n",
    "We've now seen how to use Pydantic to validate our returned types with instructor. Now let's try a more complex example\n",
    "\n",
    "Imagine you're trying to do some query parsing and you have a set of given tools\n",
    "\n",
    "1. Internet Search\n",
    "2. Database Queries\n",
    "3. Meeting Scheduler\n",
    "\n",
    "How might we represent this in a Pydantic Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17dd3c94-eae8-441b-8c3f-f799c2aa1cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions=[CalendarQuery(id=1, calendar='Personal', start_date='03-06', end_date='10-06', dependencies=[]), InternetSearch(id=2, search_query='best Japanese restaurants in Downtown Toronto', dependencies=[])]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from typing import List,Literal,Union\n",
    "from pydantic import field_validator\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "class InternetSearch(BaseModel):\n",
    "    \"\"\"\n",
    "    Model for representing an internet search query.\n",
    "    \n",
    "    \"\"\"\n",
    "    id: int = Field(..., description=\"Unique id of the query\")\n",
    "    search_query: str = Field(..., description=\"This is an internet search query that we will execute to identify relevant information.\")\n",
    "    dependencies: List[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of sub questions that need to be answered before asking this question\",\n",
    "    )\n",
    "\n",
    "class CalendarQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    A model that represents\n",
    "    \"\"\"\n",
    "    id: int = Field(..., description=\"Unique id of the query\")\n",
    "    calendar: Literal['Personal', 'Work'] = Field(..., description=\"The type of calendar (Personal or Work).\")\n",
    "    start_date: str = Field(..., description=\"The earliest date for events that we'd like to fetch for this calendar\")\n",
    "    end_date: str = Field(..., description=\"The latest date for events that we'd like to fetch for this calendar\")\n",
    "    dependencies: List[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of sub questions that need to be answered before asking this question\",\n",
    "    )\n",
    "\n",
    "    @field_validator(\"start_date\", \"end_date\")\n",
    "    def validate_date_format(cls, value):\n",
    "        try:\n",
    "            datetime.strptime(value, \"%d-%m\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Date must be in the format dd-mm\")\n",
    "        return value\n",
    "    \n",
    "\n",
    "class QueryModel(BaseModel):\n",
    "    \"\"\"\n",
    "    A list of actions to execute in order to complete the user's request\n",
    "    \"\"\"\n",
    "    actions: List[Union[InternetSearch, CalendarQuery]] = Field(..., description=\"A list of actions.\")\n",
    "\n",
    "\n",
    "def generate_actions(request: str) -> QueryModel:\n",
    "    \"\"\"\n",
    "    Generate a list of actions to schedule an appointment based on the user's request.\n",
    "    \"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        response_model=QueryModel,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a scheduling assistant capable of breaking down complex user queries into actions to be executed. Do not answer the question but instead return a list of steps in order to get enough information to answer the user's query. Always err on the side of caution.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"The date today is 27 May 2024, Monday. The user lives in Downtown Toronto and generally likes Japanese Food\"},\n",
    "            {\"role\": \"user\", \"content\": request}\n",
    "        ],\n",
    "        max_retries=3\n",
    "    )\n",
    "\n",
    "request = \"I'd like to grab dinner with Daniel sometime next week. Can you help me find some time in my calendar and some potential dinner spots?\"\n",
    "actions = generate_actions(request)\n",
    "print(actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vibecheck",
   "language": "python",
   "name": "vibecheck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
